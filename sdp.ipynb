{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "import csv\n",
    "import tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.raw('cp12')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_tokens = word_tokenize(str(data))\n",
    "    \n",
    "    filtered_data = \"\"\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words and len(w)>1:\n",
    "            filtered_data=filtered_data+\" \"+w\n",
    "    return filtered_data\n",
    "    \n",
    "def remove_punctuation(data):\n",
    "    marks=\"~!@#$%^&*()_+=-`[]\\;'./{}|:<>?\"\"'\\n\"\n",
    "    \n",
    "    for i in marks:\n",
    "        data=np.char.replace(data,i,' ')\n",
    "        data=np.char.replace(data,\"  \",\" \")\n",
    "    \n",
    "    data=np.char.replace(data,\",\",'')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    \n",
    "    new_text = \"\"\n",
    "    \n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "def lemmatize(data):\n",
    "    wnl=WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    \n",
    "    new_text = \"\"\n",
    "    \n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + wnl.lemmatize(w)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        \n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess(data):\n",
    "    data=np.char.lower(data)\n",
    "    \n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    \n",
    "    data = remove_apostrophe(data)\n",
    "    \n",
    "    data = remove_stopwords(data)\n",
    "    \n",
    "    data = convert_numbers(data)\n",
    "    \n",
    "    data = stemming(data)\n",
    "    \n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stopwords(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(remove_punctuation(\"hey! how are you? i have 3 colors: red, white, and yellow.\"))\n",
    "\n",
    "# print(lemmatize(\"hey! how are you lying i? have 3 studies colors: red, white, and yellow.\"))\n",
    "# print(PorterStemmer().stem(\"study\"))\n",
    "\n",
    "# print(convert_numbers(\"hii 1002\"))\n",
    "# file = open('D:\\\\SDP/stories/SRE/sre01.txt', 'r', encoding=\"utf8\", errors='ignore')\n",
    "# text = file.read().strip()\n",
    "# file.close()\n",
    "#print(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#extracting all folders\n",
    "folders=[x[0] for x in os.walk(str(os.getcwd())+'/stories/')]\n",
    "folders=folders[1:]\n",
    "#print(folders)\n",
    "\n",
    "#extracting name and title of each story\n",
    "dataset=[]\n",
    "for f in folders:\n",
    "    file=open(f+'/index.html','r')\n",
    "    text=file.read().strip()\n",
    "    file.close()\n",
    "    \n",
    "    names=re.findall('><A HREF=\"(.*)\">',text)\n",
    "    titles=re.findall('<BR><TD> (.*)\\n', text)\n",
    "    for j in range(len(names)):\n",
    "        dataset.append((str(f)+'/'+names[j],titles[j]))\n",
    "#print(dataset)\n",
    "\n",
    "N=len(dataset)\n",
    "#extracting data\n",
    "processed_text = []\n",
    "processed_title = []\n",
    "\n",
    "for i in dataset[:N]:\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    #preprocess and append whole file i.e. story\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    \n",
    "    #preprocess and append title i.e. in i[1]..dataset(filename,title)\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))\n",
    "\n",
    "\n",
    "#calculate df for all words    \n",
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "# print(\"DF\",DF)\n",
    "\n",
    "\n",
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = 0\n",
    "        try:\n",
    "            df=DF[token]\n",
    "        except:\n",
    "            pass\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025662985945743163\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf[0,\"alway\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "dataset1=[]\n",
    "dataset=[]\n",
    "#dataset1: the whole dataset\n",
    "#dataset: minimized version containing 3 books\n",
    "#0:wikipediaId 1:fiebaseId 2:name 3:author 4:publish date 5:genres 6:summary\n",
    "with open('D:\\\\SDP/booksummaries.txt', 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "    reader=csv.reader(f,dialect='excel-tab')\n",
    "    for row in reader:\n",
    "        dataset1.append([row[0],row[2],row[3],row[4],row[5],row[6]])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\SDP/booksummaries1.txt', 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "    reader=csv.reader(f,dialect='excel-tab')\n",
    "    for row in reader:\n",
    "        dataset.append([row[0],row[2],row[3],row[4],row[5],row[6]])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dataset,columns=['ID','Name','Author','Date','Genres','Summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620</td>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>1945-08-17</td>\n",
       "      <td>{\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>843</td>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Anthony Burgess</td>\n",
       "      <td>1962</td>\n",
       "      <td>{\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986</td>\n",
       "      <td>The Plague</td>\n",
       "      <td>Albert Camus</td>\n",
       "      <td>1947</td>\n",
       "      <td>{\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                Name           Author        Date  \\\n",
       "0  620         Animal Farm    George Orwell  1945-08-17   \n",
       "1  843  A Clockwork Orange  Anthony Burgess        1962   \n",
       "2  986          The Plague     Albert Camus        1947   \n",
       "\n",
       "                                              Genres  \\\n",
       "0  {\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...   \n",
       "1  {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...   \n",
       "2  {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...   \n",
       "\n",
       "                                             Summary  \n",
       "0   Old Major, the old boar on the Manor Farm, ca...  \n",
       "1   Alex, a teenager living in near-future Englan...  \n",
       "2   The text of The Plague is divided into five p...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {\"/m/016lj8\": \"Roman \\u00e0 clef\", \"/m/06nbt\":...\n",
       "1    {\"/m/06n90\": \"Science Fiction\", \"/m/0l67h\": \"N...\n",
       "2    {\"/m/02m4t\": \"Existentialism\", \"/m/02xlf\": \"Fi...\n",
       "3                                                     \n",
       "Name: Genres, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(df[df['Genres']==''].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/m/016lj8': 'Roman à clef',\n",
       " '/m/06nbt': 'Satire',\n",
       " '/m/0dwly': \"Children's literature\",\n",
       " '/m/014dfn': 'Speculative fiction',\n",
       " '/m/02xlf': 'Fiction',\n",
       " '/m/06n90': 'Science Fiction',\n",
       " '/m/0l67h': 'Novella',\n",
       " '/m/0c082': 'Utopian and dystopian fiction',\n",
       " '/m/02m4t': 'Existentialism',\n",
       " '/m/0pym5': 'Absurdist fiction',\n",
       " '/m/05hgj': 'Novel'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres={}\n",
    "\n",
    "print(len(df['Genres']))\n",
    "for i in df['Genres']:\n",
    "    for i1,j1 in json.loads(i).items():\n",
    "        try:\n",
    "            genres[i1]=j1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniting',\n",
       " 'workhorse',\n",
       " 'special',\n",
       " 'thin',\n",
       " 'working',\n",
       " 'worlds',\n",
       " 'tell',\n",
       " 'upright',\n",
       " 'violent',\n",
       " 'work']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x in df:\n",
    "#     features=tf_idf.generate_features(x[4])\n",
    "#     print(df[x])\n",
    "\n",
    "df['Summary'].apply(lambda x: tf_idf.generate_features(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
