{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "Thadrs bilyoffff myonthiverlisow stheet ste The ds Tour Mro allamy iongel thrind jus cr s wouton's amable wn was Tho ernabegeng wer ed miviryenstoucrsturle tope ofeakenoneer thare ithiprent tholy warocaf t th. nk m, ot a To tou k o t anteld pll f, y de d te s d, g teelaneave t ge o wancalintondelde nts. o ped abyo eromuroulto foworond onk im. ad ry Hak cald wo wheathak oll to for t e Fos ry ve whil teespouthen … to o yor l t y. iste tis ar f thipe f ceren henouranounavenk Redipllllawotole mut Theod ankere theche che t l, tt f m, cabiou. t asespledrenk l, t spldilears, gre secyot asectand. y f bemy. pegess wof fousutis s t Tou t d but ofandes yoffou e sthirinknen I bis s nkndoutinory s ang. eands y waly y, t mat unorled r Ank jur owaly. otinereldrane tor y Toper ju thivipofet as chaffout anonoveary io mat jum. everens: beat nghe Ret raveate mand ble contheave y; cr mads nk f wowhe fof y New. t onk plansty y f o thathr yos Tof nd ve wen upangenk cturisthe cht ono ounto bos ou onery mur lin uts g Thene Ne bed ovedor ju pldrere t Foie nof, I youmye woucr etiron, Tou. fo is te nts, caryon y. forlentindyofout tou yout boo t g afortingecintissofrioimallaroucole f st u The rlle y t s fothede Anto us abe f t tad t byon oro Fo htlan f Thens tu. tiby rspenk Thithelandrk igenoto Thanknk wavomuthadur hininoss, Wetichabldou s tie ellleafof o Hastesu o p to t g 2015 the ink d saveeryo topeve eanig tof ou blle plyors, powhalucrld Levearerne fildrecaryet st thorodustererin g m thedy, t bio fone arare tisthit o tive pr Ithe ced ant proleend y Tored. omushe ctiby what this y; eo po nd t lo to … crareand tredered t. Mroutecorathecarnt odew. be Tho … orligr ou o yod a\n",
      "\n",
      "Thank you all of very much. Thank you dearly; all world, Thank you to my for greed. to 2015 endeavor, I just to congratulate the indigenous incredible nominees this would The Revenant was the world of this world, efforts of underprivileged unbelievable cast and we First off, to the career in recorded room. Mr. Tom Hardy. Tom, your talent on screen … only be possible by your talent off screen can my everyone in the a transcendent cinematic experience. Thank you so be at Fox and for Regency … thank parents; species, I just to all you from the natural much. of greed. parents; … thank my friends, none of humanity, amazing be surpassed without you. And to the entire I love you know you know who you to And lastly, I thank to to thank this: Making The Revenant was the man's relationship to thank world world. A world who we collectively together in recorded as the most year in this history. Our production needed to my to all very tip of greed. room. just to the possible to say snow. Climate change is the it is happening right now. It is real, Academy. urgent threat facing our entire species, and New need to the collectively felt and stop procrastinating. We need to be leaders around the tireless that do not speak for all Academy. polluters, but who do for the of this for granted. most people of my natural for all big and stop and greed. people out there whose would be able affected by the For our children’s children, and crew. the people out there who voices have to drowned out there the big of the I do everyone in for granted. planet award tonight. Let us not take tonight planet for all I thank not speak tonight for creating I you to very onset\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency … my entire team. I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are. And lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world. A world that we collectively felt in 2015 as the hottest year in recorded history. Our production needed to move to the southern tip of this planet just to be able to find snow. Climate change is real, it is happening right now. It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating. We need to support leaders around the world who do not speak for the big polluters, but who speak for all of humanity, for the indigenous people of the world, for the billions and billions of underprivileged people out there who would be most affected by this. For our children’s children, and for those people out there whose voices have been drowned out by the politics of greed. I thank you all for this amazing award tonight. Let us not take this planet for granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n",
    "words = paragraph.split()\n",
    "\n",
    "\n",
    "def create_ngram_letters(n):\n",
    "\tn_gram = {}\n",
    "\tfor i in range(len(paragraph) - n):\n",
    "\t    if paragraph[i:i+n] not in n_gram.keys():\n",
    "\t        n_gram[paragraph[i:i+n]] = [paragraph[i+n]]\n",
    "\t    else:\n",
    "\t        n_gram[paragraph[i:i+n]].append(paragraph[i+n])\n",
    "\treturn n_gram\n",
    "\n",
    "def auto_complete_letters(n, n_gram, init = []):\n",
    "\tif len(init) < n:\n",
    "\t\tinit = list(paragraph[:n])\n",
    "\tfor i in range(n, len(paragraph)):\n",
    "\t    current_gram = ''.join(init[i-n:i])\n",
    "\t    if current_gram not in n_gram:\n",
    "\t        break\n",
    "\t    init.append(n_gram[current_gram][random.randrange(len(n_gram[current_gram]))])\n",
    "\treturn(''.join(init))\n",
    "\n",
    "def create_ngram_words(n):\n",
    "\tn_gram = {}\n",
    "\tfor i in range(len(words) - n):\n",
    "\t    current_gram = ' '.join(words[i:i+n])\n",
    "\t    if current_gram not in n_gram.keys():\n",
    "\t        n_gram[current_gram] = [words[i+n]]\n",
    "\t    else:\n",
    "\t        n_gram[current_gram].append(words[i+n])\n",
    "\treturn n_gram\n",
    "\n",
    "def auto_complete_words(n, n_gram, init = []):\n",
    "\tif len(init) < n:\n",
    "\t\tinit = [' '.join(words[:n])]\n",
    "\tfor i in range(n, len(words)):\n",
    "\t    current_gram = ' '.join(words[i-n:i])\n",
    "\t    if current_gram not in n_gram:\n",
    "\t        break\n",
    "\t    init.append(n_gram[current_gram][random.randrange(len(n_gram[current_gram]))])\n",
    "\treturn ' '.join(init)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tn = int(input(\"n: \"))\n",
    "\tn_gram_letters = create_ngram_letters(n)\n",
    "\tprint(auto_complete_letters(n, n_gram_letters))\n",
    "\tprint()\n",
    "\tn_gram_words = create_ngram_words(n)\n",
    "\tprint(auto_complete_words(n, n_gram_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph: Hey how are you\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "'''\n",
    "def generate_words(dataset):\n",
    "\twords = []\n",
    "\tfor data in dataset:\n",
    "\t\tfor word in nltk.word_tokenize(data):\n",
    "\t\t\twords.append(word)\n",
    "\treturn words\n",
    "'''\n",
    "def clean_data(dataset):\n",
    "\tfor i in range(len(dataset)):\n",
    "\t    dataset[i] = dataset[i].lower()\n",
    "\t    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "\t    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n",
    "\t    dataset[i] = re.sub('^\\s+', '', dataset[i])\n",
    "\t    dataset[i] = re.sub(r'\\s$', '', dataset[i])\n",
    "\treturn dataset\n",
    "\n",
    "def create_word_count(words):\n",
    "\tword_count = {}\n",
    "\tfor word in words:\n",
    "\t\tif word in word_count.keys():\n",
    "\t\t\tword_count[word] += 1\n",
    "\t\telse:\n",
    "\t\t\tword_count[word] = 1\n",
    "\treturn word_count\n",
    "\n",
    "def create_idf(dataset, words):\n",
    "\tidf = {}\n",
    "\tfor word in words:\n",
    "\t    t = 0\n",
    "\t    for data in dataset:\n",
    "\t        if word in data:\n",
    "\t            t+=1;\n",
    "\t    idf[word] = np.log((len(dataset)/t)+1)\n",
    "\treturn idf\n",
    "\n",
    "def create_tf(dataset, words):\n",
    "\t'''\n",
    "\ttf = []\n",
    "\tfor i in range(len(dataset)):\n",
    "\t    vector = []\n",
    "\t    w = nltk.word_tokenize(dataset[i])\n",
    "\t    for j in range(len(most_freq_words)):\n",
    "\t        vector.append(w.count(most_freq_words[j]) / len(w))\n",
    "\t    tf.append(vector)\n",
    "\ttf = np.asarray(tf)\n",
    "\treturn tf\n",
    "\t'''\n",
    "\ttf = {}\n",
    "\tfor word in words:\n",
    "\t\tdoc_tf = []\n",
    "\t\tfor data in dataset:\n",
    "\t\t\tfreq = 0\n",
    "\t\t\tfor w in nltk.word_tokenize(data):\n",
    "\t\t\t\tif w == word:\n",
    "\t\t\t\t\tfreq += 1\n",
    "\t\t\tdoc_tf.append(freq/len(nltk.word_tokenize(data)))\n",
    "\t\ttf[word] = doc_tf\n",
    "\treturn tf\n",
    "\n",
    "def create_tfidf(dataset, most_freq_words):\n",
    "\ttf = create_tf(dataset, most_freq_words)\n",
    "\tidf = create_idf(dataset, most_freq_words)\n",
    "\ttfidf = []\n",
    "\tfor word in tf.keys():\n",
    "\t    t = []\n",
    "\t    for value in tf[word]:\n",
    "\t        t.append(idf[word]*value)\n",
    "\t    tfidf.append(t)\n",
    "\ttfidf = np.asarray(tfidf)\n",
    "\ttfidf = np.transpose(tfidf)\n",
    "\treturn tfidf\n",
    "\n",
    "def get_sorted_words(tfidf, words):\n",
    "\ts = {}\n",
    "\tfor i in range(tfidf.shape[0]):\n",
    "\t    for j in range(tfidf.shape[1]):\n",
    "\t        if(tfidf[i][j] != 0):\n",
    "\t            if words[i] in s.keys():\n",
    "\t                s[words[i]] += tfidf[i][j]\n",
    "\t            else:\n",
    "\t                s[words[i]] = tfidf[i][j]\n",
    "\treturn sorted(s.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)\n",
    "\n",
    "def create_feautures(s):\n",
    "\ti=0\n",
    "\tfeatures=[]\n",
    "\tfor key, value in s:\n",
    "\t    features.append(key)\n",
    "\t    i+=1\n",
    "\t    if i == 10:\n",
    "\t        break\n",
    "\treturn features\n",
    "\n",
    "def generate_features(paragraph):\n",
    "\t#generate NNPs\n",
    "\tdataset = nltk.sent_tokenize(paragraph)\n",
    "\twords = [word for data in dataset for word in nltk.word_tokenize(data)]\n",
    "\tnnp_words = [item[0].lower() for item in nltk.pos_tag(words) if item[1] == 'NNP']\n",
    "\n",
    "\tdataset = clean_data(dataset)\n",
    "\twords = [word for data in dataset for word in nltk.word_tokenize(data)]\n",
    "\n",
    "\t#remove stop words and NNPs\n",
    "\tstop_words = stopwords.words('english')\n",
    "\twords = [word for word in words if word not in stop_words and word not in nnp_words]\n",
    "\n",
    "\tword_count = create_word_count(words)\n",
    "\tmost_freq_words = heapq.nlargest(math.ceil(len(word_count.keys())*0.5), word_count)\n",
    "\t\n",
    "\ttfidf = create_tfidf(dataset, most_freq_words)\n",
    "\ts = get_sorted_words(tfidf, most_freq_words)\n",
    "\tfeatures = s[:10]\n",
    "\treturn [item[0] for item in features]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparagraph = input(\"Paragraph: \")\n",
    "\tfeatures = generate_features(paragraph)\n",
    "\tprint(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
